# Learning representations for natural language

This class covers representation learning methods for natural language processing. Various kinds of representations will be considered: from discrete and structured representations (e.g., hidden hierarchical structure of text) to real-valued vectors (as in deep learning). The main focus will be on problems from natural language processing but most of the methods we will consider will have applications in other domains (e.g., bioinformatics, vision, information retrieval, etc). 


- **Lab 1: Evaluating word representations** The goal is to get familiar with word representation models and different techniques for evaluating them. The word representation model that we work with is Skip-gram, trained using two kinds of context: dependency-based and window-based.
